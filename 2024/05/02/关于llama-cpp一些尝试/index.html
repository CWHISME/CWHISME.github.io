<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="CWHISME" href="https://wangjiaying.top/rss.xml"><link rel="alternate" type="application/atom+xml" title="CWHISME" href="https://wangjiaying.top/atom.xml"><link rel="alternate" type="application/json" title="CWHISME" href="https://wangjiaying.top/feed.json"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="AI,llama.cpp"><link rel="canonical" href="https://wangjiaying.top/2024/05/02/%E5%85%B3%E4%BA%8Ellama-cpp%E4%B8%80%E4%BA%9B%E5%B0%9D%E8%AF%95/"><title>关于 llama.cpp 一些尝试 - AI | Jiaying's Note = CWHISME = 人不能没有梦想，也要有足够的敬畏</title><meta name="generator" content="Hexo 5.4.2"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">关于 llama.cpp 一些尝试</h1><div class="meta"><span class="item" title="创建时间：2024-05-02 00:00:00"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2024-05-02T00:00:00+08:00">2024-05-02</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>7.3k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>7 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Jiaying's Note</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="/images/coverimages/large/6833939bly1gicljitigmj20zk0m87fp.webp"></li><li class="item" data-background-image="/images/coverimages/large/9bd9b167gy1g2qm53gpn3j21hc0u0e81.webp"></li><li class="item" data-background-image="/images/coverimages/large/mmexport1588569411122.webp"></li><li class="item" data-background-image="/images/coverimages/large/6833939bly1gipexbei4hj20zk0m8npd.webp"></li><li class="item" data-background-image="/images/coverimages/large/e5221f7d85b0900837a45fb933fa34ec.webp"></li><li class="item" data-background-image="/images/coverimages/large/84910372_p0.webp"></li></ul></div><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div></header><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/AI/" itemprop="item" rel="index" title="分类于 AI"><span itemprop="name">AI</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://wangjiaying.top/2024/05/02/%E5%85%B3%E4%BA%8Ellama-cpp%E4%B8%80%E4%BA%9B%E5%B0%9D%E8%AF%95/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/../img/avator"><meta itemprop="name" content="WangJiaYing"><meta itemprop="description" content="人不能没有梦想，也要有足够的敬畏, 己所不欲，勿施于人"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="CWHISME"></span><div class="body md" itemprop="articleBody"><h1 id="前言"><a class="anchor" href="#前言">#</a> 前言</h1><p>2024 年 5 月 1 日：研究 Python 绿色化执行环境，安装 OpenInterpreter，各种折腾了一天。</p><p>今天 (2024 年 5 月 2 日) 开始想试试下 llama.cpp，之前 LMStudio 用了一阵子了，感觉虽然挺方便，但毕竟又套了一层，而且有些参数也是不直接提供的。<br>另外 koboldcpp 也试用了下，UI 界面其实挺简单，但竟然没提供设置自动保存 (需要手动存储，然后手动加载才能恢复设置 -- 特指客户端的界面，它提供的 Web 端会自动存储在浏览器)。</p><p>所以想了下，既然大家都是从 llama.cpp 上发展过来的，还不如直接上 llama.cpp。</p><p>llama.cpp 项目几乎是目前所有本地运行 GGUF LLM 模型的源头，同时虽然不像 LMStudio 那样直接提供 Rocm 版本运行程序，但貌似也可以直接编译，而且还有 Vulkan 一类一样可以被 AMD GPU 加速运行 (虽然可能效率比不上 ROCM)，但是试用了下，感觉速度还蛮可以的。于是今天就开始鼓捣起流程。</p><h1 id="尝试"><a class="anchor" href="#尝试">#</a> 尝试</h1><p>llama.cpp 毕竟是纯命令行，所以就想着写些命令行脚本，然后放环境变量，方便随时执行。</p><p>昨天通过研究非安装的 Python 环境 (也就是完全不影响主机端，相当于做成绿色软件模式)，然后完全独立地安装了 OpenInterpreter，试用下来感觉命令行有时候还是挺方便的。</p><p>于是今天的想法也是通过写命令行批处理来试试，然后一键执行。<br>期间有许多模型涉及到不同参数，又研究如何传递，另外还有 llama.cpp 的 server 模式等。</p><p>然后一路看文档，一路实验和研究。<br>最后脚本写好了，执行起来才发现 llama.cpp 的 server 其实也还是个 WIP 版，用起来跟相比 LMStudio 的 Server 还挺多问题。</p><p>例如，尝试了下 SillyTavern 完全没法用，生成一堆乱码一样的东西，而且 SillyTavern 一乱，其它地方的客户端再去请求这个 Server，收到的也是乱码了！<br>也就是说 llama.cpp 的 server 不同客户端连过去执行还会混掉？</p><h1 id="系统提示"><a class="anchor" href="#系统提示">#</a> 系统提示</h1><p>先是怀疑指令提示的问题 (也即是 --chat-template)，比如模型的指令提示不兼容，于是查了文档，发现目前版本的指令提示已经会自动从模型取了：<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dnZXJnYW5vdi9sbGFtYS5jcHAvd2lraS9UZW1wbGF0ZXMtc3VwcG9ydGVkLWJ5LWxsYW1hX2NoYXRfYXBwbHlfdGVtcGxhdGU=">https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template</span><br><img data-src="/blogimages/2024/2024-05-12/1.png" alt="image.png"><br>而且只支持预定义模板。<br>反正研究了下，感觉没啥用。<br>然后在文档上找到个 system_prompt 的描述，也就是 --system-prompt-file FNAME 命令，于是猜测是不是系统提示被盖掉了的原因？(感觉又不像，客户端怎么会)<br>不过试了下似乎有改善，也就是好像不会混了：即 SillyTavern 请求了乱码之后，其它客户端再请求也会是好的，但尝试去掉 --system-prompt-file 之后，貌似也是好了.... 所以想不通。<br>SillyTavern 反正也还是乱码，有的模型在 llama.cpp server 本身输出是好的，但是在 ChatGPT Sidebar 里边输出最后还老跟一个 &quot;&lt;|im_end|&gt;&quot; 符号，不知道咋回事。</p><h1 id="反提示"><a class="anchor" href="#反提示">#</a> 反提示</h1><p>结果查询，LLM 回答的最后跟一个 &quot;&lt;|im_end|&gt;&quot; 可能跟反提示有关。<br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FiZXRsZW4vbGxhbWEtY3BwLXB5dGhvbi9pc3N1ZXMvNzU5">https://github.com/abetlen/llama-cpp-python/issues/759</span><br><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2gyb2FpL2gyb2dwdC9pc3N1ZXMvMTIwMA==">https://github.com/h2oai/h2ogpt/issues/1200</span><br>打开 LMStudio 的 Preset 目录，以 ChatML 为例：</p><figure class="highlight json"><figcaption data-lang="JSON"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="2"></td><td><pre>  <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"ChatML"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="3"></td><td><pre>  <span class="token property">"system_prompt"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    <span class="token property">"input_prefix"</span><span class="token operator">:</span> <span class="token string">"&lt;|im_end|>\n&lt;|im_start|>user\n"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    <span class="token property">"input_suffix"</span><span class="token operator">:</span> <span class="token string">"&lt;|im_end|>\n&lt;|im_start|>assistant\n"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token property">"antiprompt"</span><span class="token operator">:</span> <span class="token punctuation">[</span></pre></td></tr><tr><td data-num="7"></td><td><pre>      <span class="token string">"&lt;|im_start|>"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="8"></td><td><pre>      <span class="token string">"&lt;|im_end|>"</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    <span class="token punctuation">]</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    <span class="token property">"pre_prompt_prefix"</span><span class="token operator">:</span> <span class="token string">"&lt;|im_start|>system\n"</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    <span class="token property">"pre_prompt_suffix"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token property">"pre_prompt"</span><span class="token operator">:</span> <span class="token string">"Perform the task to the best of your ability."</span></pre></td></tr><tr><td data-num="13"></td><td><pre>  <span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="14"></td><td><pre><span class="token punctuation">&#125;</span></pre></td></tr></table></figure><p>其中 antiprompt 应该就是反提示了，听说这个字段主要是用于停止搜索使用。<br>我测试出问题的模型 (SOVL_Llama3_8B-Q8_0-imat.gguf) 是基于 llama3 基础上训练的，怀疑微调训练的时候用了 ChatML 格式的提示，导致如上截图，llama.cpp 读取内嵌提示信息不匹配导致。<br>于是把 LMStudio ChatML 预设拷贝了一份出来，改了下然后加入加载模型参数的命令 system_prompt 上：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token string">"%LLAMACPP_PATH%/server"</span> <span class="token parameter variable">--mlock</span> <span class="token parameter variable">-c</span> <span class="token number">4096</span> <span class="token parameter variable">-ngl</span> %GPU_LAYER_COUNT% <span class="token parameter variable">-m</span> %FILE_NAME% <span class="token parameter variable">-spf</span> preset/minipreset.json --log-format text <span class="token parameter variable">--port</span> <span class="token number">1357</span></pre></td></tr></table></figure><p>运行之后，作为 llama3 竟然直接回答中文了：<br><img data-src="/blogimages/2024/2024-05-12/2.png" alt="image.png"><br>... 虽然在 slider 上，回答最后还是会加上 &quot;&lt;|im_end|&gt;&quot; .....<br>SillyTavern 也还是乱码：<br><img data-src="/blogimages/2024/2024-05-12/3.png" alt="image.png"></p><h1 id="原因"><a class="anchor" href="#原因">#</a> 原因</h1><p>后来偶然又用 LMStudio 试了下，结果发现如果不手动选取 llama3 的提示， LMStudio 同样会有这个输出问题.....<br><img data-src="/blogimages/2024/2024-05-12/4.png" alt="image.png"><br>于是再次使用 --chat-template llama3 加载，结果竟然好了..... 再仔细一看上面的日志截图， llama.cpp 默认加载的模板，可不就是 ChatML 格式的吗.... 这应该是训练的时候用了 ChatML 的提示，导致被量化到量化文件里了。<br>我怀疑基座模型和指令模型的还有一个区别就是，在于是否把提示也炼进去了？<br>这里又出现一个问题：没法解释为什么 llama.cpp server 自带的 web 是好的？<br>反正换成 llama3 模板后， Slider 就好了，llama.cpp server 日志也可以看到 build_in 变量变成了 false：<br><img data-src="/blogimages/2024/2024-05-12/5.png" alt="image.png"></p><p>然后 SillyTavern 还是不行，而且乱码之后经常还是会影响到其它客户端，最后在 llama.cpp server 文档上注意到这句话：<br><img data-src="/blogimages/2024/2024-05-12/6.png" alt="image.png"><br><s>搞半天别人客户端难道还可以直接修改系统提示，而且是公共的？！</s><br>试了下 koboldcpp Vulkan 加速也是乱码.... 这是后端的就有问题？！<br>最后在文档上看到一张图：<br><img data-src="/blogimages/2024/2024-05-12/7.png" alt="image.png"><br>Vulkan 兼容性倒数第二....<br>乱码的，换成 CPU 模式，至少就能跑了。</p><h1 id="总结"><a class="anchor" href="#总结">#</a> 总结</h1><p>五一节花时间研究这个，最后灰溜溜的.... 不愧是急速开发中的项目，这东西真的还不能直接用啊... 要不然还是算了。<br>前两天，Ubuntu24.04 在 4 月 25 号发布之后，也是还兴冲冲地装了 Linux 系统，结果早上装上，当天就又换回去了 —— 装个输入法都费劲，特别是由于系统比较新，一堆依赖直接已经都没了...... 本来都不留后路地想换系统，把原本 Windows 系统盘直接格掉重装的，结果还是... 唉，最后也是导致折腾得厉害.... 所以有时候还是得留点后路 —— 虽然系统重装之后，发现比之前带了一堆陈旧软件的时候似乎快了些，特别是开关机的时候 (估计是因为没装那么多驱动，比如华硕的奥创中心)。</p><hr><p>更新 (2024 年 5 月 3 日)：</p><ul><li>SillyTavern 有 Stop words，也就是说可以规避系统提示问题 (因为是在高级设置所以之前没发现...)，也有自定义 System Prompt 的功能，也是在高级设置里面...</li><li>llama.cpp 本身 Vulkan 兼容性不提，关于正常情况是好的的模型，SillyTavern 发出的消息计算后是乱码的原因倒也有些线索了，llama3 仅在 SillyTavern 请求时回复一堆乱码，应该也算是破案了一点了：</li><li><img data-src="/blogimages/2024/2024-05-12/8.png" alt="image.png"></li><li>开启 verbose 模式，可以看到，接收端已经乱了，估摸着还是编码问题导致，至于究竟是 llama.cpp server 解析端有问题还是 SillyTavern 发送端有问题，个人倾向于解析端 —— 毕竟其它第三方 server 测试至少正常对话的模型，SillyTavern 也是好的，估计 llama.cpp 用纯 C 实现的解析方式对编码兼容性可能还有点问题。</li><li>不止是中文，默认的英文提示也会出现问题：</li><li><img data-src="/blogimages/2024/2024-05-12/9.png" alt="image.png"></li><li>对比下原文：</li><li><img data-src="/blogimages/2024/2024-05-12/10.png" alt="image.png"></li></ul><hr><p>继续更新：</p><ul><li>又仔细研究了下 llama.cpp server 的文档，最后把怀疑的目光指向了两个参数：<ul><li>-cb, --cont-batching: Enable continuous batching (a.k.a dynamic batching). Default: disabled</li><li>--slots-endpoint-disable: To disable slots state monitoring endpoint. Slots state may contain user data, prompts included.</li></ul></li><li>这两个参数看起来都非常可疑：<ul><li>第一个动态连续批处理，看解释好像是说提高吞吐量的，怀疑是不是导致不同客户端请求混淆的原因？而且文档说是默认关闭，实际上运行程序帮助信息又显示是默认开启：<img data-src="/blogimages/2024/2024-05-12/11.png" alt="image.png"></li><li>第二个参数是禁用 slots 功能，官方文档没有更多解释，只这样说可能包含一些用户数据、提示什么的，不过看其它地方好像是指 “某个端点上提供的扩展接口，允许用户插入额外的设备或功能来增强端点的性能或功能” 这种意思。那么，是否就是这个功能导致 SillyTavern 一些额外参数将其影响到了？</li></ul></li><li>应该确认了，连续处理确实对不同客户端都有影响，新鲜出炉的 BUG (也不能说新鲜出炉，应该还是有一阵子了)：<ul><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dnZXJnYW5vdi9sbGFtYS5jcHAvaXNzdWVzLzcwNTI=">https://github.com/ggerganov/llama.cpp/issues/7052</span></li><li><img data-src="/blogimages/2024/2024-05-12/12.png" alt="image.png"></li></ul></li></ul><hr><ul><li>再次尝试 --slots-endpoint-disable ，该参数对于乱码情况无效。</li><li>实测如果仅仅只是开场少数几个 Token，能够正常回答，当 Token 量较大时，无论如何都会乱码。或者新开几个对话，直接就炸了，疯狂输出乱码，还有可能导致整个 Server 持续 100% GPU 占用停不下来......</li></ul><h1 id="更新"><a class="anchor" href="#更新">#</a> 更新</h1><p>2024 年 5 月 12 日更新：</p><ul><li>llama.cpp server b2860 已经修复上述问题</li><li>所以上述猜测可能也不正确</li></ul><p>那么就顺便帖一下之前写的 bat 代码，主要分为两部分，一部分是作为工作的核心，另外一个作为简单参数的调用：<br>llamacpp.bat (核心)：</p><figure class="highlight bash"><figcaption data-lang="bash"></figcaption><table><tr><td data-num="1"></td><td><pre>@echo off</pre></td></tr><tr><td data-num="2"></td><td><pre>setlocal enabledelayedexpansion</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre>rem 第一个参数是相对路径</pre></td></tr><tr><td data-num="5"></td><td><pre>rem 第二个参数是 GPU 卸载层数</pre></td></tr><tr><td data-num="6"></td><td><pre>rem 可选参数为 <span class="token builtin class-name">set</span> <span class="token assign-left variable">CHAT_TEMPLATE</span><span class="token operator">=</span>llama3</pre></td></tr><tr><td data-num="7"></td><td><pre>rem -    是否有强制聊天模板参数，没有就走默认，否则走强制；这是因为默认的受量化内嵌参数影响，可能解析出错</pre></td></tr><tr><td data-num="8"></td><td><pre>rem -    chatml,llama2,llama3,monarch,gemma,orion,openchat,vicuna,vicuna-orca,deepseek,command-r,zephyr</pre></td></tr><tr><td data-num="9"></td><td><pre>rem -    CHAT_TEMPLATE 模板参见：https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template</pre></td></tr><tr><td data-num="10"></td><td><pre>rem 其它额外参数：</pre></td></tr><tr><td data-num="11"></td><td><pre>rem -   上下文长度<span class="token punctuation">(</span>默认4096<span class="token punctuation">)</span>：CONTEXT_SIZE<span class="token operator">=</span><span class="token number">4096</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre><span class="token builtin class-name">set</span> <span class="token assign-left variable">FILE_NAME</span><span class="token operator">=</span>%~dp0<span class="token punctuation">..</span>/LLM_Models/%1</pre></td></tr><tr><td data-num="14"></td><td><pre>@REM <span class="token builtin class-name">set</span> <span class="token string">"SYSTEM_PROMPT=-spf config-presets/chatml.preset.json"</span></pre></td></tr><tr><td data-num="15"></td><td><pre></pre></td></tr><tr><td data-num="16"></td><td><pre>echo.</pre></td></tr><tr><td data-num="17"></td><td><pre>echo.</pre></td></tr><tr><td data-num="18"></td><td><pre><span class="token builtin class-name">echo</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span></pre></td></tr><tr><td data-num="19"></td><td><pre><span class="token builtin class-name">echo</span> <span class="token assign-left variable">model</span><span class="token operator">=</span>%1</pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre><span class="token keyword">if</span> not exist <span class="token string">"%FILE_NAME%"</span> <span class="token punctuation">(</span> </pre></td></tr><tr><td data-num="22"></td><td><pre>    <span class="token builtin class-name">echo</span> The <span class="token function">file</span> %FILE_NAME% does not exist.</pre></td></tr><tr><td data-num="23"></td><td><pre>    goto :end</pre></td></tr><tr><td data-num="24"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre></pre></td></tr><tr><td data-num="26"></td><td><pre>rem 主要用来检测卸载 GPU 层的参数的</pre></td></tr><tr><td data-num="27"></td><td><pre><span class="token keyword">if</span> <span class="token string">"%2"</span><span class="token operator">==</span><span class="token string">""</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="28"></td><td><pre>    rem 没传就全卸载</pre></td></tr><tr><td data-num="29"></td><td><pre>    <span class="token builtin class-name">set</span> <span class="token string">"GPU_LAYER_COUNT=10000"</span></pre></td></tr><tr><td data-num="30"></td><td><pre>    <span class="token builtin class-name">echo</span> Using gpu layers: <span class="token number">100</span>%%</pre></td></tr><tr><td data-num="31"></td><td><pre><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="32"></td><td><pre>    <span class="token builtin class-name">set</span> <span class="token string">"GPU_LAYER_COUNT=%2"</span></pre></td></tr><tr><td data-num="33"></td><td><pre>    <span class="token builtin class-name">echo</span> Using gpu layers: %2</pre></td></tr><tr><td data-num="34"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>rem 获取文件目录路径</pre></td></tr><tr><td data-num="37"></td><td><pre><span class="token keyword">for</span> /F <span class="token string">"tokens=* delims="</span> %%a <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">"%FILE_NAME%"</span><span class="token punctuation">)</span> <span class="token keyword">do</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="38"></td><td><pre>    <span class="token builtin class-name">set</span> <span class="token string">"FILE_PATH=%%~dpa"</span> </pre></td></tr><tr><td data-num="39"></td><td><pre>    <span class="token builtin class-name">break</span></pre></td></tr><tr><td data-num="40"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="41"></td><td><pre></pre></td></tr><tr><td data-num="42"></td><td><pre>rem <span class="token builtin class-name">echo</span> File Path: %FILE_PATH%</pre></td></tr><tr><td data-num="43"></td><td><pre></pre></td></tr><tr><td data-num="44"></td><td><pre>rem 检查文件路径是否同时存在多模态文件</pre></td></tr><tr><td data-num="45"></td><td><pre><span class="token keyword">if</span> defined FILE_PATH <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="46"></td><td><pre>    <span class="token keyword">for</span> /F <span class="token string">"tokens=*"</span> %%i <span class="token keyword">in</span> <span class="token punctuation">(</span><span class="token string">'dir /B /A:-D "%FILE_PATH%\"'</span><span class="token punctuation">)</span> <span class="token keyword">do</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="47"></td><td><pre>        <span class="token builtin class-name">echo</span> %%i <span class="token operator">|</span> <span class="token function">find</span> <span class="token string">"mmproj"</span> <span class="token operator">></span>nul</pre></td></tr><tr><td data-num="48"></td><td><pre>        <span class="token keyword">if</span> not errorlevel <span class="token number">1</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="49"></td><td><pre>            <span class="token builtin class-name">set</span> <span class="token string">"MMPROJ_PATH=%FILE_PATH%%%i"</span></pre></td></tr><tr><td data-num="50"></td><td><pre>            <span class="token builtin class-name">echo</span> found mmproj: %FILE_PATH%%%i</pre></td></tr><tr><td data-num="51"></td><td><pre>            <span class="token builtin class-name">break</span></pre></td></tr><tr><td data-num="52"></td><td><pre>        <span class="token punctuation">)</span></pre></td></tr><tr><td data-num="53"></td><td><pre>    <span class="token punctuation">)</span></pre></td></tr><tr><td data-num="54"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="55"></td><td><pre></pre></td></tr><tr><td data-num="56"></td><td><pre>rem <span class="token builtin class-name">echo</span> <span class="token assign-left variable">mmproj</span><span class="token operator">=</span>%MMPROJ_PATH%</pre></td></tr><tr><td data-num="57"></td><td><pre><span class="token builtin class-name">echo</span> <span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span></pre></td></tr><tr><td data-num="58"></td><td><pre>echo.</pre></td></tr><tr><td data-num="59"></td><td><pre>echo.</pre></td></tr><tr><td data-num="60"></td><td><pre></pre></td></tr><tr><td data-num="61"></td><td><pre>rem 搜索 llama.cpp 路径</pre></td></tr><tr><td data-num="62"></td><td><pre><span class="token keyword">for</span> /D /r <span class="token string">"%~dp0/.."</span> %%d <span class="token keyword">in</span> <span class="token punctuation">(</span>llama-*<span class="token punctuation">)</span> <span class="token keyword">do</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="63"></td><td><pre>    <span class="token builtin class-name">set</span> <span class="token string">"LLAMACPP_PATH=%%d"</span></pre></td></tr><tr><td data-num="64"></td><td><pre>    goto :found</pre></td></tr><tr><td data-num="65"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="66"></td><td><pre></pre></td></tr><tr><td data-num="67"></td><td><pre>:found</pre></td></tr><tr><td data-num="68"></td><td><pre>rem 是否有强制聊天模板参数，没有就走默认，否则走强制；这是因为默认的受量化内嵌参数影响，可能解析出错</pre></td></tr><tr><td data-num="69"></td><td><pre>rem <span class="token builtin class-name">echo</span> %CHAT_TEMPLATE%</pre></td></tr><tr><td data-num="70"></td><td><pre><span class="token keyword">if</span> defined CHAT_TEMPLATE <span class="token builtin class-name">set</span> <span class="token string">"CHAT_TEMPLATE=--chat-template %CHAT_TEMPLATE%"</span></pre></td></tr><tr><td data-num="71"></td><td><pre>rem <span class="token builtin class-name">echo</span> %CHAT_TEMPLATE%</pre></td></tr><tr><td data-num="72"></td><td><pre>rem <span class="token keyword">else</span> <span class="token builtin class-name">set</span> %CHAT_TEMPLATE<span class="token operator">=</span><span class="token string">""</span>%</pre></td></tr><tr><td data-num="73"></td><td><pre>@REM 上下文长度</pre></td></tr><tr><td data-num="74"></td><td><pre><span class="token builtin class-name">echo</span> %CONTEXT_SIZE%</pre></td></tr><tr><td data-num="75"></td><td><pre><span class="token keyword">if</span> defined CONTEXT_SIZE <span class="token punctuation">(</span> </pre></td></tr><tr><td data-num="76"></td><td><pre>    <span class="token builtin class-name">set</span> <span class="token string">"CONTEXT_SIZE=-c %CONTEXT_SIZE%"</span></pre></td></tr><tr><td data-num="77"></td><td><pre><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="78"></td><td><pre><span class="token builtin class-name">set</span> <span class="token string">"CONTEXT_SIZE=-c 4096"</span> <span class="token punctuation">)</span></pre></td></tr><tr><td data-num="79"></td><td><pre><span class="token builtin class-name">echo</span> %CONTEXT_SIZE%</pre></td></tr><tr><td data-num="80"></td><td><pre></pre></td></tr><tr><td data-num="81"></td><td><pre><span class="token keyword">if</span> defined LLAMACPP_PATH <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="82"></td><td><pre>    <span class="token builtin class-name">echo</span> found llamacpp: %LLAMACPP_PATH%</pre></td></tr><tr><td data-num="83"></td><td><pre>    rem powershell <span class="token parameter variable">-Command</span> Write-Host Found directory: %FOUND_DIR% <span class="token parameter variable">-ForegroundColor</span> Red</pre></td></tr><tr><td data-num="84"></td><td><pre>    <span class="token keyword">if</span> defined MMPROJ_PATH <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="85"></td><td><pre>        <span class="token builtin class-name">echo</span> auto with multimodal projector<span class="token punctuation">[</span>mmproj<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="86"></td><td><pre>        <span class="token string">"%LLAMACPP_PATH%/server"</span> <span class="token parameter variable">--mlock</span> <span class="token parameter variable">-c</span> <span class="token number">4096</span> <span class="token parameter variable">-ngl</span> %GPU_LAYER_COUNT% <span class="token parameter variable">-m</span> %FILE_NAME% <span class="token parameter variable">--port</span> <span class="token number">1357</span> <span class="token punctuation">;</span>--mmproj %MMPROJ_PATH% </pre></td></tr><tr><td data-num="87"></td><td><pre>    <span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="88"></td><td><pre>        <span class="token builtin class-name">echo</span> <span class="token keyword">in</span> regular mode because not found multimodal projector<span class="token punctuation">[</span>mmproj<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="89"></td><td><pre>        <span class="token string">"%LLAMACPP_PATH%/server"</span> <span class="token parameter variable">--mlock</span> %CONTEXT_SIZE% <span class="token parameter variable">-ngl</span> %GPU_LAYER_COUNT% <span class="token parameter variable">-m</span> %FILE_NAME% %CHAT_TEMPLATE% %SYSTEM_PROMPT% <span class="token parameter variable">--port</span> <span class="token number">1357</span> --slots-endpoint-disable --log-disable --no-kv-offload</pre></td></tr><tr><td data-num="90"></td><td><pre>        @REM  <span class="token parameter variable">-v</span> --ubatch-size <span class="token number">64</span> </pre></td></tr><tr><td data-num="91"></td><td><pre>        @REM --no-kv-offload :默认 KV Cahche 卸载到 CPU，加上这个就会一直处于 GPU，内存要求更高</pre></td></tr><tr><td data-num="92"></td><td><pre>    <span class="token punctuation">)</span></pre></td></tr><tr><td data-num="93"></td><td><pre>    rem call <span class="token string">"%FOUND_DIR%\script.bat"</span></pre></td></tr><tr><td data-num="94"></td><td><pre>    goto :end</pre></td></tr><tr><td data-num="95"></td><td><pre><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token punctuation">(</span></pre></td></tr><tr><td data-num="96"></td><td><pre>    <span class="token builtin class-name">echo</span> No matching LLAMACPP directory found.</pre></td></tr><tr><td data-num="97"></td><td><pre><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="98"></td><td><pre></pre></td></tr><tr><td data-num="99"></td><td><pre>:end</pre></td></tr></table></figure><p>调用，例如：</p><figure class="highlight basic"><figcaption data-lang="BASIC"></figcaption><table><tr><td data-num="1"></td><td><pre>@echo <span class="token keyword">off</span></pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token function">set</span> CONTEXT_SIZE<span class="token operator">=</span><span class="token number">8000</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">call</span> llamacpp.bat Meta\Llama3\Hermes<span class="token operator">-</span><span class="token number">2</span><span class="token operator">-</span>Pro<span class="token operator">-</span>Llama<span class="token operator">-</span><span class="token number">3</span><span class="token operator">-</span><span class="token number">8</span>B<span class="token operator">-</span>Q8_0.gguf</pre></td></tr></table></figure><hr><p>所以最后准备自己写代码了，直接调 llama.cpp 来用。</p><h1 id="参考文档"><a class="anchor" href="#参考文档">#</a> 参考文档</h1><ul><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dnZXJnYW5vdi9sbGFtYS5jcHAvdHJlZS9tYXN0ZXIvZXhhbXBsZXMvc2VydmVyI2NoYW5nZS1zeXN0ZW0tcHJvbXB0LW9uLXJ1bnRpbWU=">https://github.com/ggerganov/llama.cpp/tree/master/examples/server#change-system-prompt-on-runtime</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dnZXJnYW5vdi9sbGFtYS5jcHAvd2lraS9UZW1wbGF0ZXMtc3VwcG9ydGVkLWJ5LWxsYW1hX2NoYXRfYXBwbHlfdGVtcGxhdGU=">https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FiZXRsZW4vbGxhbWEtY3BwLXB5dGhvbi9pc3N1ZXMvNzU5">https://github.com/abetlen/llama-cpp-python/issues/759</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2gyb2FpL2gyb2dwdC9pc3N1ZXMvMTIwMA==">https://github.com/h2oai/h2ogpt/issues/1200</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9tZWRpdW0uY29tL0Bhbm1vbHRhbHdhci9wcm9tcHQtcHJlLWZpeGluZy1mb3ItbGxtLWVmZmljaWVudC16ZXJvLXNob3QtcHJvbXB0aW5nLWZhNGM3NzNlNDM3NQ==">https://medium.com/@anmoltalwar/prompt-pre-fixing-for-llm-efficient-zero-shot-prompting-fa4c773e4375</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dnZXJnYW5vdi9sbGFtYS5jcHAvaXNzdWVzLzYzOTE=">https://github.com/ggerganov/llama.cpp/issues/6391</span></li><li><span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2dnZXJnYW5vdi9sbGFtYS5jcHAvaXNzdWVzLzcwNTI=">https://github.com/ggerganov/llama.cpp/issues/7052</span></li></ul><div class="tags"><a href="/tags/AI/" rel="tag"><i class="ic i-tag"></i> AI</a> <a href="/tags/llama-cpp/" rel="tag"><i class="ic i-tag"></i> llama.cpp</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2024-05-12 19:44:48" itemprop="dateModified" datetime="2024-05-12T19:44:48+08:00">2024-05-12</time></span></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>WangJiaYing <i class="ic i-at"><em>@</em></i>CWHISME</li><li class="link"><strong>本文链接：</strong> <a href="https://wangjiaying.top/2024/05/02/%E5%85%B3%E4%BA%8Ellama-cpp%E4%B8%80%E4%BA%9B%E5%B0%9D%E8%AF%95/" title="关于 llama.cpp 一些尝试">https://wangjiaying.top/2024/05/02/关于llama-cpp一些尝试/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2023/12/24/%E4%BD%BF%E7%94%A8Mac%E7%B3%BB%E7%BB%9F%E4%B8%8D%E5%BE%97%E4%B8%8D%E5%81%9A%E7%9A%84%E9%85%8D%E7%BD%AE/" itemprop="url" rel="prev" data-background-image="&#x2F;images&#x2F;coverimages&#x2F;large&#x2F;76327148_p0.webp" title="使用Mac系统需要做的一点配置记录"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 其它</span><h3>使用Mac系统需要做的一点配置记录</h3></a></div><div class="item right"><a href="/2024/05/26/Avalonia%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/" itemprop="url" rel="next" data-background-image="&#x2F;images&#x2F;coverimages&#x2F;large&#x2F;1a9ad97e06a50ddbcbfbe3445560f6cd_3_2_art.webp" title="Avalonia一些记录"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> Avalonia</span><h3>Avalonia一些记录</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B0%9D%E8%AF%95"><span class="toc-number">2.</span> <span class="toc-text">尝试</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%8F%90%E7%A4%BA"><span class="toc-number">3.</span> <span class="toc-text">系统提示</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%8D%E6%8F%90%E7%A4%BA"><span class="toc-number">4.</span> <span class="toc-text">反提示</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8E%9F%E5%9B%A0"><span class="toc-number">5.</span> <span class="toc-text">原因</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0"><span class="toc-number">7.</span> <span class="toc-text">更新</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E6%A1%A3"><span class="toc-number">8.</span> <span class="toc-text">参考文档</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/2024/05/02/%E5%85%B3%E4%BA%8Ellama-cpp%E4%B8%80%E4%BA%9B%E5%B0%9D%E8%AF%95/" rel="bookmark" title="关于llama.cpp一些尝试">关于llama.cpp一些尝试</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="WangJiaYing" data-src="/images/../img/avator"><p class="name" itemprop="name">CWHISME</p><div class="description" itemprop="description">己所不欲，勿施于人</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">129</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">24</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">35</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL2N3aGlzbWU=" title="https:&#x2F;&#x2F;github.com&#x2F;cwhisme"><i class="ic i-github"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9jd2hpc21lLTMzL3Bvc3Rz" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;cwhisme-33&#x2F;posts"><i class="ic i-zhihu"></i></span> <span class="exturl item email" data-url="bWFpbHRvOmN3aGlzbWVAMTI2LmNvbQ==" title="mailto:cwhisme@126.com"><i class="ic i-envelope"></i></span> <a href="/atom.xml" title="&#x2F;atom.xml" class="item feedback"><i class="ic i-heart"></i></a></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友链</a></li><li class="item"><a href="/statistics/" rel="section"><i class="ic i-clock"></i>统计</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>关于</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2023/12/24/%E4%BD%BF%E7%94%A8Mac%E7%B3%BB%E7%BB%9F%E4%B8%8D%E5%BE%97%E4%B8%8D%E5%81%9A%E7%9A%84%E9%85%8D%E7%BD%AE/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/05/26/Avalonia%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E7%90%86%E8%AE%BA%E7%A0%94%E7%A9%B6/" title="分类于 理论研究">理论研究</a></div><span><a href="/2017/02/17/%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/" title="【工具】寻路算法[A*的简析与实现]">【工具】寻路算法[A*的简析与实现]</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%90%86%E8%AE%BA%E7%A0%94%E7%A9%B6/" title="分类于 理论研究">理论研究</a></div><span><a href="/2022/11/17/%E6%B5%8B%E8%AF%95%E6%89%8B%E5%8A%A8%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/" title="简单测试手动垃圾回收">简单测试手动垃圾回收</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Unity3D/" title="分类于 Unity3D">Unity3D</a></div><span><a href="/2021/07/28/%E4%BD%BF%E7%94%A8MagicCloth%E5%81%9A%E8%A1%A3%E7%89%A9%E6%88%96%E5%A4%B4%E5%8F%91%E7%89%A9%E7%90%86%E6%95%88%E6%9E%9C/" title="使用MagicCloth做衣物或头发物理效果">使用MagicCloth做衣物或头发物理效果</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment count_3"></ul></div></div><div class="status"><div class="copyright">&copy; 2015 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">WangJiaYing @ Jiaying's Note</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">760k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">11:31</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0NXSElTTUUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2024/05/02/关于llama-cpp一些尝试/",favicon:{show:"（●´3｀●）",hide:"(´Д｀)"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,fancybox:!0,justifiedGallery:!0,jquery:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.staticfile.net/pace/1.2.4/pace.min.js"></script><script src="https://cdn.staticfile.net/pjax/0.2.8/pjax.min.js"></script><script src="https://cdn.staticfile.net/animejs/3.2.2/anime.min.js"></script><script src="https://cdn.staticfile.net/algoliasearch/4.17.0/algoliasearch-lite.umd.js"></script><script src="https://cdn.staticfile.net/instantsearch.js/4.54.1/instantsearch.production.min.js"></script><script src="https://cdn.staticfile.net/lozad.js/1.16.0/lozad.min.js"></script><script src="https://cdn.staticfile.net/quicklink/2.3.0/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="/js/DateTimeAfeterCalc.js" async></script><script src="https://fastly.jsdelivr.net/gh/CWHISME/live2d_api_models@master/autoload.js" async></script></body></html>